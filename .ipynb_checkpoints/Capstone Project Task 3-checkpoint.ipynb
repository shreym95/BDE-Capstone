{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6add3749",
   "metadata": {},
   "source": [
    "# Fractal - Analytics Vidhya, Crossover Big Data Engineering Training Batch\n",
    "### Capstone Assessment - Task 3, Shrey Marwaha, 19th June 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc084525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import min, max, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c98653e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"Capstone Task 3\") \\\n",
    "                    .config(\"spark.sql.warehouse.dir\", \"/user/ana002673/warehouse\") \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d51650b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g01.itversity.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Capstone Task 3</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0dd9d5b5f8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b418761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab5f957f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use shreycapstonedb;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "607aa891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-----------+\n",
      "|       database|           tableName|isTemporary|\n",
      "+---------------+--------------------+-----------+\n",
      "|shreycapstonedb|       coursedetails|      false|\n",
      "|shreycapstonedb|coursedetails_sta...|      false|\n",
      "|shreycapstonedb|coursedetails_sta...|      false|\n",
      "|shreycapstonedb|studentcoursecomp...|      false|\n",
      "|shreycapstonedb|studentcoursecomp...|      false|\n",
      "|shreycapstonedb|                test|      false|\n",
      "|shreycapstonedb|               test1|      false|\n",
      "|shreycapstonedb|               test2|      false|\n",
      "|shreycapstonedb|       test2_staging|      false|\n",
      "|shreycapstonedb|               test3|      false|\n",
      "+---------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tables = spark.sql(\"show tables;\")\n",
    "tables.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719bb15b",
   "metadata": {},
   "source": [
    "### Task 1 - Creating dataframes and reading data from Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46022eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+----------+----------+\n",
      "|courseid|               title|competency|complexity|coursetype|\n",
      "+--------+--------------------+----------+----------+----------+\n",
      "|   C0001|Certificate in Cl...| Technical|     Basic|     Cloud|\n",
      "|   C0002|Certificate in Vi...| Technical|     Basic|     Cloud|\n",
      "|   C0003|Diploma in Inform...|  Security|  Advanced|     Cloud|\n",
      "|   C0004|BE (Hons) in CSE ...|    Domain|  Advanced|     Cloud|\n",
      "|   C0005|BTech in Computer...|    Domain|  Advanced|     Cloud|\n",
      "+--------+--------------------+----------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "course_details = spark.read.table(\"COURSEDETAILS\")\n",
    "course_details.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a1e94c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+--------------+-----+-------------+\n",
      "|studentsid|courseid|  examdate|attendedstatus|marks|       result|\n",
      "+----------+--------+----------+--------------+-----+-------------+\n",
      "|     S0001|   C0001|2019-02-17|      Attended|   70|    Qualified|\n",
      "|     S0298|   C0008|2019-02-24|      Attended|   70|    Qualified|\n",
      "|     S0297|   C0007|2019-02-23|        Absent|    0|Not Qualified|\n",
      "|     S0296|   C0030|2019-02-20|      Attended|   70|    Qualified|\n",
      "|     S0291|   C0013|2019-03-01|      Attended|   85|    Qualified|\n",
      "+----------+--------+----------+--------------+-----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student_status = spark.read.table(\"StudentCourseCompletionStatus\")\n",
    "student_status.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0243ddf9",
   "metadata": {},
   "source": [
    "### Task 2 - Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70e6209",
   "metadata": {},
   "source": [
    "#### 1. Total number of students per result category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afe1f7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>result</th><th>count</th></tr>\n",
       "<tr><td>Not Qualified</td><td>149</td></tr>\n",
       "<tr><td>Qualified</td><td>151</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------+-----+\n",
       "|       result|count|\n",
       "+-------------+-----+\n",
       "|Not Qualified|  149|\n",
       "|    Qualified|  151|\n",
       "+-------------+-----+"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_status.groupBy('result').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca9a994",
   "metadata": {},
   "source": [
    "#### 2. Total number of students absent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd5068d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_status.where(student_status['attendedstatus']=='Absent').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbd7477",
   "metadata": {},
   "source": [
    "#### 3. Maximum, Minimum and Average Marks scored by students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af32d59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------+-------------+\n",
      "|studentsid|Minimum Marks|Maximum Marks|Average Marks|\n",
      "+----------+-------------+-------------+-------------+\n",
      "|     S0010|            0|            0|          0.0|\n",
      "|     S0049|           77|           77|         77.0|\n",
      "|     S0281|           70|           70|         70.0|\n",
      "|     S0242|            0|            0|          0.0|\n",
      "|     S0120|           70|           70|         70.0|\n",
      "|     S0030|           70|           70|         70.0|\n",
      "|     S0161|           70|           70|         70.0|\n",
      "|     S0111|            0|            0|          0.0|\n",
      "|     S0086|            0|            0|          0.0|\n",
      "|     S0128|           91|           91|         91.0|\n",
      "|     S0169|           91|           91|         91.0|\n",
      "|     S0292|           86|           86|         86.0|\n",
      "|     S0069|           86|           86|         86.0|\n",
      "|     S0139|            0|            0|          0.0|\n",
      "|     S0202|            0|            0|          0.0|\n",
      "|     S0154|           85|           85|         85.0|\n",
      "|     S0142|           70|           70|         70.0|\n",
      "|     S0112|            0|            0|          0.0|\n",
      "|     S0289|            0|            0|          0.0|\n",
      "|     S0272|            0|            0|          0.0|\n",
      "+----------+-------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student_status.groupBy('studentsid').agg(min('marks').alias('Minimum Marks'), \\\n",
    "                                         max('marks').alias('Maximum Marks'), \\\n",
    "                                         avg('marks').alias('Average Marks')\n",
    "                                        ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15907c9a",
   "metadata": {},
   "source": [
    "#### 4. Overall Maximum, Minimum and Average Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bd5cd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+------------------+\n",
      "|Minimum Marks|Maximum Marks|     Average Marks|\n",
      "+-------------+-------------+------------------+\n",
      "|            0|           92|40.013333333333335|\n",
      "+-------------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student_status.agg(min('marks').alias('Minimum Marks'), \\\n",
    "                   max('marks').alias('Maximum Marks'), \\\n",
    "                   avg('marks').alias('Average Marks') \\\n",
    "                  ).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
